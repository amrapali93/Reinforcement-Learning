{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Implementing the Tic Tac Toe game for an agent playing first or second based on a random selection. \n# Approximate reinforcement techniques like Policy gradient and temporal difference is used for the same \n# Learns the optimal first move while playing first or second\n# Learns that getting 3 X's is good but does not block O's \n# state = board positions, action = next move, method = Policy gradient and temporal difference \n# Performance gets better with more training ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c2e3ed547f266fe64a8cae1d4049a5f36abd563e"
      },
      "cell_type": "code",
      "source": "import numpy as np \nimport pandas as pd ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9baf1bf5d083495099da465f353a1caa0ce0d58d",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Decide the game points based on a given state of the board \n# +1 if agent wins, -1 if opponent wins and zero otherwise \ndef game_points(matrix):\n    agent_win = np.array([1,1,1])\n    opponent_win = np.array([-1,-1,-1])\n    if ((matrix[0:3] == agent_win).all() | (matrix[0::4] == agent_win).all() | (matrix[0::3] == agent_win).all() | (matrix[1::3] == agent_win).all()\n        | (matrix[2::3] == agent_win).all() | (matrix[2:8:2] == agent_win).all() | (matrix[3:6] == agent_win).all() | (matrix[6:9] == agent_win).all()):\n        return 1\n    elif ((matrix[0:3] == opponent_win).all() | (matrix[0::4] == opponent_win).all() | (matrix[0::3] == opponent_win).all() | (matrix[1::3] == opponent_win).all()\n        | (matrix[2::3] == opponent_win).all() | (matrix[2:8:2] == opponent_win).all() | (matrix[3:6] == opponent_win).all() | (matrix[6:9] == opponent_win).all()):\n        return -1\n    else:\n        return 0\n\n# Function for selecting a move for the agent/opponent (player) based on a given policy (greedy/random) \n# player = 'agent' or 'opponent' | random = True or False \n# compile models before running the functions \ndef First_player_selection(player,random):\n    available_positions = np.where(matrix==0)[0] \n    if random == True:\n        selection = np.random.choice(available_positions)\n    else:\n        selection = np.argmax(model_agentF2.predict(matrix.reshape(1,9)))\n        if selection not in available_positions:\n            selection = np.random.choice(available_positions)\n    if player == 'agent':\n        matrix[selection] = 1\n    else:\n        matrix[selection] =-1\n    return selection\n\ndef Second_player_selection(player,random):\n    available_positions = np.where(matrix==0)[0] \n    if random == True:\n        selection = np.random.choice(available_positions)\n    else:\n        selection = np.argmax(model2.predict(matrix.reshape(1,9)))\n        if selection not in available_positions:\n            selection = np.random.choice(available_positions)\n    if player == 'agent':\n        matrix[selection] = 1\n    else:\n        matrix[selection] =-1\n    return selection",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "72642be9469adeefc440b3d88e9d11db6895e985"
      },
      "cell_type": "code",
      "source": "## Neural network implementation of approximate value function of the states \n# opponent first\n# model to learn state values with agent playing second \nfrom keras import Sequential\nfrom keras.layers import Dense\nmodel = Sequential()\nmodel.add(Dense(1,input_shape=(9,)))\nmodel.compile(loss='mse', optimizer='adam', metrics=['mae'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "3196377544e59b9998e395c85dc3398cefc4bc3d"
      },
      "cell_type": "code",
      "source": "## agent first \n# model to learn state values with agent playing First \nfrom keras import Sequential\nfrom keras.layers import Dense\nmodel_agentF = Sequential()\nmodel_agentF.add(Dense(1,input_shape=(9,)))\nmodel_agentF.compile(loss='mse', optimizer='adam', metrics=['mae'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6fed819cd8de53bbcf42ca9cc2da142747b7ab7c",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Train model \n# opponent first\nnum_episodes = 500  # (increase for better training)\nfor i in range(num_episodes):\n    matrix = np.zeros(9)\n    turn = 0 \n    opponent_selections= []\n    agent_selections = []\n    while ((game_points(matrix) == 0) &  (turn < 8)): \n        # opponent makes a move \n        opponent_selections.append(First_player_selection('opponent',True))\n        turn += 1\n        # state before agent takes an action \n        initial_state = matrix\n        # if game not over agent makes a move \n        if ((turn <=7) and (game_points(matrix) == 0)):\n            agent_selections.append(Second_player_selection('agent',True))\n            turn += 1\n            turn\n        # state after agent takes an action \n        after_state = matrix\n        # reward of being in that state \n        ri = game_points(initial_state)\n        ra = game_points(after_state)\n        #target = ra\n        target = (ra+ri) + 0*(model.predict(initial_state.reshape(1,9)))\n        model.fit(initial_state.reshape(1,9),target , epochs=10, verbose=0) ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "90bf3aeb095a983b488edec10f57415e885ca39e"
      },
      "cell_type": "code",
      "source": "# train model\n# agent first\nnum_episodes = 500\nfor i in range(num_episodes):\n    matrix = np.zeros(9)\n    turn = 0 \n    opponent_selections= []\n    agent_selections = []\n    while ((game_points(matrix) == 0) &  (turn < 8)): \n        # state before agent takes an action \n        initial_state = matrix\n        # agent makes a move \n        agent_selections.append(First_player_selection('agent',True))\n        turn += 1\n        # state after agent takes an action \n        after_state = matrix\n        # if game not over opponent makes a move \n        if ((turn <=7) and (game_points(matrix) == 0)):\n            opponent_selections.append(Second_player_selection('opponent',True))\n            turn += 1\n        # reward of being in a state \n        ri = game_points(initial_state)\n        ra = game_points(after_state)\n        #target = ra\n        target = (ra+ri) + 0*(model_agentF.predict(initial_state.reshape(1,9)))\n        model_agentF.fit(initial_state.reshape(1,9),target , epochs=10, verbose=0)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d760f1ff47267f1deb3953973bfd3f99d5765b83",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# test values of different states \n#initial_state1 = np.array([-1,-1,-1,1,1,-1,1,0,-1])\ninitial_state1 = np.array([1,0,0,0,1,0,-1,0,1])\ninitial_state2 = np.array([-1,0,0,0,-1,0,1,0,-1])\nmodel_agentF.predict(initial_state1.reshape(1,9)), model_agentF.predict(initial_state2.reshape(1,9))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Neural Network implementation to learn action value functions with agent as second player\n# opponent first \nfrom keras import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import SGD\nsgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\nmodel2 = Sequential()\nmodel2.add(Dense(9,input_shape=(9,), activation = 'linear'))\nmodel2.compile(loss='mse', optimizer='sgd', metrics=['mse'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "6ef70c138c38a9a4f85af8df4138b782ac6a9932"
      },
      "cell_type": "code",
      "source": "# Neural Network implementation for action value functions with agent as first player\n#agent first \nfrom keras import Sequential\nfrom keras.layers import Dense\nmodel_agentF2 = Sequential()\nmodel_agentF2.add(Dense(9,input_shape=(9,), activation = 'linear'))\nmodel_agentF2.compile(loss='mse', optimizer='sgd', metrics=['mse'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6ca2f7423201ec52292387663d23ce0893542ca4"
      },
      "cell_type": "code",
      "source": "model_agentF2.get_config()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b4872ae2d79474ee1cc1bdb4cac2fcbbd3f317b8",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Train model \n# opponent first \ny = 0.95\neps = 0.5\ndecay_factor = 0.999\nnum_episodes = 2000\na= 0\nfor i in range(num_episodes):\n    matrix = np.zeros(9)\n    turn = 0 \n    opponent_selections= []\n    agent_selections = []\n    eps *= decay_factor\n    initial_state = np.zeros(9)\n    after_state = np.ones(9)\n    input_vector = np.zeros([1,9])\n    output_vector = np.zeros([1,9])\n    r=0\n    while ((game_points(matrix) == 0) &  (turn < 8)):\n        opponent_selections.append(First_player_selection('opponent',True))\n        turn += 1\n        initial_state = matrix \n        if ((turn <=7) and (game_points(initial_state) == 0)):  \n            if (np.random.random() > eps):\n                agent_selections.append(Second_player_selection('agent',False))\n            else:\n                agent_selections.append(Second_player_selection('agent',True))\n            turn += 1\n        after_state = matrix \n        if (not(initial_state == after_state).all()):\n            target = (game_points(after_state)+(model.predict(after_state.reshape(1,9))))[0][0]  # update target with reward and predicted value of next step \n            target_vec = model2.predict(initial_state.reshape(1,9))[0]  # this models prediction\n            target_vec[agent_selections[-1]] += target   # update value of selected action with calculated target value \n            # store the state and target vector to train model \n            input_vector= np.vstack([input_vector,initial_state])\n            output_vector = np.vstack([output_vector,target_vec])\n    model2.fit(input_vector, output_vector, epochs=100, verbose=0)  # train model with inputs from each episode or game ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "062d2749c8b05d2024f80bf9a2517ed82bbe6a85"
      },
      "cell_type": "code",
      "source": "#Train model for actions to be taken with\n# agent first \ny = 0.95\neps = 0.5\ndecay_factor = 0.999\nnum_episodes = 2000\na= 0\nfor i in range(num_episodes):\n    matrix = np.zeros(9)\n    turn = 0 \n    opponent_selections= []\n    agent_selections = []\n    eps *= decay_factor\n    initial_state = np.zeros(9)\n    after_state = np.ones(9)\n    input_vector = np.zeros([1,9])\n    output_vector = np.zeros([1,9])\n    r=0\n    while ((game_points(matrix) == 0) &  (turn < 8)):\n        initial_state = matrix\n        if (np.random.random() > eps):\n            agent_selections.append(First_player_selection('agent',False))\n        else:\n            agent_selections.append(First_player_selection('agent',True))\n        turn += 1\n        after_state = matrix\n        if ((turn <=7) and (game_points(after_state) == 0)):\n            opponent_selections.append(Second_player_selection('opponent',True))\n            turn += 1            \n        target = (game_points(after_state)+(model_agentF.predict(after_state.reshape(1,9))))[0][0]\n        target_vec = model_agentF2.predict(initial_state.reshape(1,9))[0]\n        target_vec[agent_selections[-1]] += target\n        # store the state and target vector to train model \n        input_vector= np.vstack([input_vector,initial_state])\n        output_vector = np.vstack([output_vector,target_vec])\n    model_agentF2.fit(input_vector, output_vector, epochs=10, verbose=0)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ab1c6a3d792f38ec7ea55bb5965dbfa1537e049d",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "initial_state1 = np.array([ -1, -1,  0,  1,  0, 0,  0, 0, 0])\ninitial_state2 = np.array([-1,1,-1,-1,1,0,0,0,1])\nmodel_agentF2.predict(initial_state1.reshape(1,9)), model2.predict(initial_state2.reshape(1,9))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "53c7eda2f9ccbe1a5c88812e04c417d9ca0f1eb0"
      },
      "cell_type": "code",
      "source": "# plot tic tac toe board \nimport matplotlib.pyplot as plt \n%matplotlib inline\ndef plot_matrix(selection, agent):\n    for x in range(4):\n        plt.plot([x, x], [0,3], 'k')\n    for y in range(4):\n        plt.plot([0, 3], [y,y], 'k')\n    if agent == False:\n        s = plt.plot((np.mod(selection,3) + 0.5),(2.5-int(selection/3)),\n                          'o',markersize=30, markeredgecolor=(1,0,0), markerfacecolor='w', markeredgewidth=2)\n    else:\n        s = plt.plot((np.mod(selection,3) + 0.5),(2.5-int(selection/3)),\n                          'x',markersize=30, markeredgecolor=(0,1,0), markerfacecolor='w', markeredgewidth=2)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "314a4b3fb6cd3df3a7697a6e64dceadc03d05967"
      },
      "cell_type": "code",
      "source": "# tic tac toe simulation using the above trained models \nmatrix = np.zeros(9)\ndef play_game():\n    turn =0\n    opponent_selections= []\n    agent_selections = []\n    agent_first=True\n    #if np.random.random() > 0.5:\n        #agent_first = True\n    print('agent_first', agent_first)\n    while ((game_points(matrix) == 0) &  (turn < 8)): \n        \n        if agent_first == True:\n            agent_selections.append(First_player_selection('agent',False))\n            turn +=1\n            if ((turn <=7) and (game_points(matrix) == 0)):\n                opponent_selections.append(Second_player_selection('opponent',True))\n                turn+=1\n        else:\n            opponent_selections.append(First_player_selection('opponent',True))\n            turn +=1\n            if ((turn <=7) and (game_points(matrix) == 0)):\n                agent_selections.append(Second_player_selection('agent',False))\n                turn+=1\n                \n        plot_matrix(opponent_selections[-1],False)\n        plot_matrix(agent_selections[-1],True)\n    print(matrix, agent_selections,opponent_selections)\n    if game_points(matrix) == 1:\n        print('agent wins')\n    elif game_points(matrix) == -1:\n        print('opponent wins')\n    else:\n        print('draw')\nplay_game()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "094f169c2d1bc1358606850f14b0cb8294f10cff"
      },
      "cell_type": "code",
      "source": "# the agent has learned that best move for starting second is center \n# Learned to get 3 X's in a row \n# does not know to block 3 o's is some cases ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f5cc4cf44cf0a4cbb8bdf62faa76a9f80f4565da"
      },
      "cell_type": "code",
      "source": "model.save('my_model_agent_second.h5')\nmodel_agentF2.save('my_model_agent_first.h5')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "980c2a0d5b2c9e6a88b3c3a8e39735911d6e52c5",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "from keras.models import load_model\nmodel3 = load_model('my_model.h5')",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}