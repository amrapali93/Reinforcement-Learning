{
  "cells": [
    {
      "metadata": {
        "trusted": true,
        "_uuid": "378fef8006dece1ce34c728c444ca9ccf1152133"
      },
      "cell_type": "code",
      "source": "# Reinforcement learning to play Tic Tac Toe as a first player using Policy gradient and Monte Carlo \n# state - board positions, actions - next move, method - Policy Gradient and Monte Carlo \n# the agent has learned that best move for starting first is corner  \n# Learned to get 3 X's in a row\n# does not know to block 3 o's is some cases \n# does not know to get 3 X's in least possible moves \n# if O inbetween two X's then agent starts to select other positions that can give 3 X's\n# Performace gets better with more training ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np \nimport pandas as pd ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9baf1bf5d083495099da465f353a1caa0ce0d58d"
      },
      "cell_type": "code",
      "source": "# +1 if agent wins, -1 if opponent wins and zero otherwise \ndef game_points(matrix):\n    agent_win = np.array([1,1,1])\n    opponent_win = np.array([-1,-1,-1])\n    if ((matrix[0:3] == agent_win).all() | (matrix[0::4] == agent_win).all() | (matrix[0::3] == agent_win).all() | (matrix[1::3] == agent_win).all()\n        | (matrix[2::3] == agent_win).all() | (matrix[2:8:2] == agent_win).all() | (matrix[3:6] == agent_win).all() | (matrix[6:9] == agent_win).all()):\n        return 1\n    elif ((matrix[0:3] == opponent_win).all() | (matrix[0::4] == opponent_win).all() | (matrix[0::3] == opponent_win).all() | (matrix[1::3] == opponent_win).all()\n        | (matrix[2::3] == opponent_win).all() | (matrix[2:8:2] == opponent_win).all() | (matrix[3:6] == opponent_win).all() | (matrix[6:9] == opponent_win).all()):\n        return -1\n    else:\n        return 0\n\ndef First_player_selection(player,random):\n    available_positions = np.where(matrix==0)[0] \n    if random == True:\n        selection = np.random.choice(available_positions)\n    else:\n        selection = int(model_agentF2.predict(matrix.reshape(1,9))[0])\n        if selection not in available_positions:\n            selection = np.random.choice(available_positions)\n    if player == 'agent':\n        matrix[selection] = 1\n    else:\n        matrix[selection] =-1\n    return selection\n\ndef Second_player_selection(player,random):\n    available_positions = np.where(matrix==0)[0] \n    if random == True:\n        selection = np.random.choice(available_positions)\n    else:\n        selection = np.argmax(ada1.predict(matrix.reshape(1,9)))\n        if selection not in available_positions:\n            selection = np.random.choice(available_positions)\n    if player == 'agent':\n        matrix[selection] = 1\n    else:\n        matrix[selection] =-1\n    return selection",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3196377544e59b9998e395c85dc3398cefc4bc3d"
      },
      "cell_type": "code",
      "source": "## agent first  \nfrom keras import Sequential\nfrom keras.layers import Dense\nmodel_agentF = Sequential()\nmodel_agentF.add(Dense(1,input_shape=(9,)))\nmodel_agentF.compile(loss='mse', optimizer='adam', metrics=['mae'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "90bf3aeb095a983b488edec10f57415e885ca39e"
      },
      "cell_type": "code",
      "source": "# agent first\nnum_episodes = 500\nfor i in range(num_episodes):\n    matrix = np.zeros(9)\n    turn = 0 \n    opponent_selections= []\n    agent_selections = []\n    while ((game_points(matrix) == 0) &  (turn < 8)): \n        # state before agent takes an action \n        initial_state = matrix\n        # agent makes a move \n        agent_selections.append(First_player_selection('agent',True))\n        turn += 1\n        # state after agent takes an action \n        after_state = matrix\n        # if game not over opponent makes a move \n        if ((turn <=7) and (game_points(matrix) == 0)):\n            opponent_selections.append(Second_player_selection('opponent',True))\n            turn += 1\n        # reward of being in a state \n        ri = game_points(initial_state)\n        ra = game_points(after_state)\n        #target = ra\n        target = (ra+ri) + 0*(model_agentF.predict(initial_state.reshape(1,9)))\n        model_agentF.fit(initial_state.reshape(1,9),target , epochs=10, verbose=0)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d760f1ff47267f1deb3953973bfd3f99d5765b83"
      },
      "cell_type": "code",
      "source": "# test values of different states \n#initial_state1 = np.array([-1,-1,-1,1,1,-1,1,0,-1])\ninitial_state1 = np.array([1,0,0,0,1,0,-1,0,1])\ninitial_state2 = np.array([-1,0,0,0,-1,0,1,0,-1])\nmodel_agentF.predict(initial_state1.reshape(1,9)), model_agentF.predict(initial_state2.reshape(1,9))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6ef70c138c38a9a4f85af8df4138b782ac6a9932"
      },
      "cell_type": "code",
      "source": "# compile a model to take input the current board state and output an action \n#agent first \nfrom keras import Sequential\nfrom keras.layers import Dense\nmodel_agentF2 = Sequential()\nmodel_agentF2.add(Dense(1,input_shape=(9,), activation = 'linear'))\nmodel_agentF2.compile(loss='mae', optimizer='sgd', metrics=['mae'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "062d2749c8b05d2024f80bf9a2517ed82bbe6a85"
      },
      "cell_type": "code",
      "source": "# train model only for episodes where agent wins. Hence, the model learns and predicts the actions required to win in a given state\n# agent first \ny = 0.95\neps = 0.5\ndecay_factor = 0.999\nnum_episodes = 3000\nsample= 0\nfor i in range(num_episodes):\n    matrix = np.zeros(9)\n    turn = 0 \n    opponent_selections= []\n    agent_selections = []\n    eps *= decay_factor\n    initial_state = np.zeros(9)\n    after_state = np.ones(9)\n    input_vector = np.zeros([1,9])\n    r=0\n    while ((game_points(matrix) == 0) &  (turn < 8)):\n        initial_state = matrix\n        if (np.random.random() > eps):\n            agent_selections.append(First_player_selection('agent',False))\n        else:\n            agent_selections.append(First_player_selection('agent',True))\n        turn += 1\n        after_state = matrix\n        if ((turn <=7) and (game_points(after_state) == 0)):\n            opponent_selections.append(Second_player_selection('opponent',True))\n            turn += 1            \n        # store the state and target vector to train model \n        if (game_points(after_state) == 0):\n            input_vector= np.vstack([input_vector,initial_state])\n    target = game_points(matrix)\n    output_vector = np.array(agent_selections)[:,np.newaxis] \n    if target == 1:\n        sample +=1 \n        model_agentF2.fit(input_vector, output_vector, epochs=100, verbose=0)\nprint(sample)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ab1c6a3d792f38ec7ea55bb5965dbfa1537e049d"
      },
      "cell_type": "code",
      "source": "initial_state1 = np.array([ -1, 1,  1,  0,  1, 0,  0, 0, 0])\nada.predict(initial_state1.reshape(1,9))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fbd1f33e20cc07289769095f42859218569dc773"
      },
      "cell_type": "code",
      "source": "initial_state, after_state, input_vector, output_vector",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "53c7eda2f9ccbe1a5c88812e04c417d9ca0f1eb0"
      },
      "cell_type": "code",
      "source": "import matplotlib.pyplot as plt \ndef plot_matrix(selection, agent):\n    for x in range(4):\n        plt.plot([x, x], [0,3], 'k')\n    for y in range(4):\n        plt.plot([0, 3], [y,y], 'k')\n    if agent == False:\n        s = plt.plot((np.mod(selection,3) + 0.5),(2.5-int(selection/3)),\n                          'o',markersize=30, markeredgecolor=(1,0,0), markerfacecolor='w', markeredgewidth=2)\n    else:\n        s = plt.plot((np.mod(selection,3) + 0.5),(2.5-int(selection/3)),\n                          'x',markersize=30, markeredgecolor=(0,1,0), markerfacecolor='w', markeredgewidth=2)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "314a4b3fb6cd3df3a7697a6e64dceadc03d05967",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "%matplotlib inline\n# simple greedy selection method \nmatrix = np.zeros(9)\ndef play_game():\n    turn =0\n    opponent_selections= []\n    agent_selections = []\n    agent_first=True\n    #if np.random.random() > 0.5:\n        #agent_first = True\n    print('agent_first', agent_first)\n    while ((game_points(matrix) == 0) &  (turn < 8)): \n        \n        if agent_first == True:\n            agent_selections.append(First_player_selection('agent',False))\n            turn +=1\n            if ((turn <=7) and (game_points(matrix) == 0)):\n                opponent_selections.append(Second_player_selection('opponent',True))\n                turn+=1\n        else:\n            opponent_selections.append(First_player_selection('opponent',True))\n            turn +=1\n            if ((turn <=7) and (game_points(matrix) == 0)):\n                agent_selections.append(Second_player_selection('agent',False))\n                turn+=1\n                \n        plot_matrix(opponent_selections[-1],False)\n        plot_matrix(agent_selections[-1],True)\n    print(matrix, agent_selections,opponent_selections)\n    if game_points(matrix) == 1:\n        print('agent wins')\n    elif game_points(matrix) == -1:\n        print('opponent wins')\n    else:\n        print('draw')\nplay_game()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "094f169c2d1bc1358606850f14b0cb8294f10cff"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a22ec444cacaa38ecb08f5d8e4cd6e3614b824ba"
      },
      "cell_type": "code",
      "source": "(model_agentF2.predict(matrix.reshape(1,9))[0])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "f5cc4cf44cf0a4cbb8bdf62faa76a9f80f4565da"
      },
      "cell_type": "code",
      "source": "model2.save('my_model_agent_second.h5')\nmodel_agentF2.save('my_model_agent_first.h5')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "980c2a0d5b2c9e6a88b3c3a8e39735911d6e52c5",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "from keras.models import load_model\nmodel3 = load_model('my_model.h5')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "517dd8f25ea20e15ff94472e594412f6d7c4a57c",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "from subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}