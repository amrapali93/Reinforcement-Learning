{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Example 4.2: Jack's Car Rental from chapter 4: Dynamic programming of Sutton and Bartoâ€™s book: Reinforcement Learning: An Introduction\n# car inventory management using dynamic programming. \n#The demand and suppy of the inventory is assumed to have a poisson distribution \n# the enviornment dynamic are decided as per Markov Decision Process \nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom math import *",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# at loc1 on an average in one day 3 cars are requested and 3 are returned and for loc2 4 are requested and 2 are returned\nexpected_request_loc1 = 3\nexpected_return_loc1 = 3\nexpected_request_loc2 = 4 \nexpected_return_loc2 = 2\n# other variables (assumed)\nmax_cars = 20\nmax_moved = 5 \ndiscount_rate = 0.9",
      "execution_count": 15,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c6ef557cf30cc5093971db6a5e5daec265d35019"
      },
      "cell_type": "code",
      "source": "# states (number of cars at each of the two locations at a given time)\nstates=[]\nfor i in range(max_cars+1):\n    for j in range(max_cars+1):\n        states.append([i,j])",
      "execution_count": 14,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d6eb6d825bfa2f572b457a1552f3b4185e5f3218"
      },
      "cell_type": "code",
      "source": "#policy # initialised to zero i.e probability of any action from a given state is zero. hence no action is taken. No cars moved \n# deterministic policy \npolicy = np.zeros((max_cars + 1, max_cars + 1))",
      "execution_count": 13,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e55ec5db46b95a3657fcfa3ae66d5857b9db1404"
      },
      "cell_type": "code",
      "source": "# actions # positive for loc1=>loc2 and negative for loc22=>loc1\nactions = np.arange(-max_moved,max_moved+1)",
      "execution_count": 5,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fb653c7026c58bacdc8f0598555bf1928c88a216"
      },
      "cell_type": "code",
      "source": "# Reward\ncredit = 10\nloss = -2 ",
      "execution_count": 6,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "73e46d64023d8091238d8daae16134672b9ff6db"
      },
      "cell_type": "code",
      "source": "# Poisson distribution \n# Cars requested or returned at each location on a given day\n# probability that there are n customers or n returns on a given day\n#p(n) = ((lamda)^n/n!)*e^(-lamda)  #n = 0,1,2,3,4,5,6,7,8,9,10 as expected value(lamda) lie around 3 & 4 hence probability of n> 10 is very low\ndef poisson(lam):\n    temp ={}\n    for key in range(11):\n        temp[key] = (np.exp(-lam) * pow(lam, key)) / factorial(key)\n    return temp",
      "execution_count": 7,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5e497c76f6a8ddd2826f8c9e41c03cbec850ba96"
      },
      "cell_type": "code",
      "source": "cars_requested_loc1 = poisson(expected_request_loc1)\ncars_returned_loc1 = poisson(expected_return_loc1)\ncars_requested_loc2 = poisson(expected_request_loc2)\ncars_returned_loc2 = poisson(expected_return_loc2)",
      "execution_count": 8,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1c8a52ad64e322c768049b134e5b654299d37d0b"
      },
      "cell_type": "code",
      "source": "#value function\ncurrentStateVal = np.zeros((max_cars + 1, max_cars + 1))\nnewStateVal = np.zeros((max_cars + 1, max_cars + 1))",
      "execution_count": 9,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b73f3da2ac5da329f3aec2a030214c7700960071"
      },
      "cell_type": "code",
      "source": "# reward +10$ for fulfilling a request and -2$ loss for every unfulfilled request\ndef reward(state,cars_requested):\n    if state >= cars_requested:\n        reward = 10\n    else:\n        reward = -2 \n    return reward    ",
      "execution_count": 10,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b918c4ffdcbd77f89120ef3aded9ac59f3614a5e"
      },
      "cell_type": "code",
      "source": "# policy evaluation\n#state_start_2 = state_end_1 + actions + cars_returned_1\n#state_end_1 = state_start_1 - cars_requested_1\nimprovePolicy = False\n\nfor i in range(len(states)):\n    newStateVal[states[i][0], states[i][1]] = new_values(states[i], policy[states[i][0], states[i][1]], currentStateVal)",
      "execution_count": 43,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1ecb59cf689ce888a6e192d6eed01c99daeba235"
      },
      "cell_type": "code",
      "source": "# stop improving state value function if the difference between new value and current value is small \nif np.sum(np.absolute(newStateVal - currentStateVal)) < 1e-4:\n    currentStateVal = newStateVal.copy()\n    improvePolicy = True",
      "execution_count": 47,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a32e220af52ae31b8c778efb2a36ee093bc6c900"
      },
      "cell_type": "code",
      "source": "improvePolicy",
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 48,
          "data": {
            "text/plain": "True"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "397d608b62b5bb2eebbe74f730b8dbf7a955933d"
      },
      "cell_type": "code",
      "source": "# iterate over all cases of returns and requests to get net probability of next states for a given state and action, p(s',r|s,r) \ndef new_values(current_state, action, currentStateVal):\n    new_value = 0\n    requests_com = [0,0]\n    requests_incom = [0,0]\n    rewards = 0\n    state_DayStart = [0,0]\n    next_state = np.array([0,0])\n    for i in range(11):  # request loop\n        for j in range(11):\n            requests_com[0] = min(i,current_state[0])\n            requests_com[1] = min(j,current_state[1]) \n            if i-current_state[0] >=0:\n                requests_incom[0] = i-current_state[0]\n            else:\n                requests_incom[0] = 0 \n                \n            if j-current_state[1] >=0:\n                requests_incom[1] = i-current_state[1]\n            else:\n                requests_incom[1] = 0 \n             \n            rewards = ((requests_com[0] + requests_com[1]) * 10) - ((requests_incom[0] + requests_incom[1]) *2)   \n            prob = cars_requested_loc1[i] * cars_requested_loc2[j]\n            for k in range(11):  # return loop \n                for l in range(11):                   \n                    # cars to start the day #next days start. Action is taken on current state. this is the intermediate state after action is taken \n                    state_DayStart[0] = current_state[0] - action  \n                    state_DayStart[1] = current_state[1] + action \n                    # cars at end of day # next day end state. This is the final state upon which next action will be taken\n                    next_state[0] = min(state_DayStart[0] - requests_com[0] + k , max_cars)\n                    next_state[1] = min(state_DayStart[1] - requests_com[1] + l , max_cars)\n                    prob = (cars_returned_loc1[k] * cars_returned_loc2[l])*prob\n                    new_value += prob*(rewards + discount_rate*(currentStateVal[next_state[0],next_state[1]]))\n        \n    return new_value    ",
      "execution_count": 11,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7a825e607c822d416739bc7b39eccb3d7fc49c71"
      },
      "cell_type": "code",
      "source": "# policy improvement \nif improvePolicy == True:\n    newPolicy = np.zeros((max_cars + 1, max_cars + 1))\n    for i, j in states:\n        actionReturns = []\n        for action in actions:\n            if ((action >= 0 and i >= action) or (action < 0 and j >= np.absolute(action))):\n                actionReturns.append(new_values([i, j], action, currentStateVal))\n            else:\n                actionReturns.append(-float('inf'))\n        bestAction = np.argmax(actionReturns)\n        newPolicy[i, j] = actions[bestAction]\n    policyChanges = np.sum(newPolicy != policy)\n    policy = newPolicy\n    improvePolicy = False",
      "execution_count": 49,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "d6c7329024133eee1d58ce086c1ebb0fb7d44ca4"
      },
      "cell_type": "code",
      "source": "newPolicy",
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 50,
          "data": {
            "text/plain": "array([[ 0., -1., -2., -3., -4., -5., -5., -5., -5., -5., -4., -3., -4.,\n        -4., -4., -5., -5., -5., -5., -5., -5.],\n       [ 1., -1., -2., -3., -4., -5., -5., -5.,  1.,  1.,  1.,  1.,  1.,\n         1.,  1.,  1.,  1.,  1., -5., -5., -5.],\n       [ 2., -1., -2., -3., -4., -5., -5.,  2.,  2.,  2.,  2.,  2.,  2.,\n         2.,  2.,  2.,  2.,  2., -5., -5., -5.],\n       [ 3.,  3., -2., -3., -4., -5., -5.,  3.,  3.,  3.,  3.,  3.,  3.,\n         3.,  3.,  3.,  3., -5., -5., -5., -5.],\n       [ 4.,  4.,  4., -3., -4., -5.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,\n         4.,  4.,  4.,  4., -5., -5., -5., -5.],\n       [ 5.,  5.,  5., -3., -4., -5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,\n         5.,  5.,  5., -4., -5., -5., -5., -5.],\n       [ 5., -1., -2., -3., -4., -5., -5.,  5.,  5.,  5.,  5.,  5., -2.,\n        -2., -3., -3., -4., -4., -5., -5., -5.],\n       [ 5., -1., -2., -3., -4., -5., -5., -5.,  5.,  5., -1., -1., -2.,\n        -2., -3., -3., -4., -4., -5., -5., -5.],\n       [ 5., -1., -2., -3., -4., -5., -5., -5.,  1.,  1.,  0., -1., -1.,\n        -2., -2., -3., -3., -4., -4., -5., -5.],\n       [ 5., -1., -2., -3., -4., -5., -5., -5.,  2.,  1.,  1.,  0., -1.,\n        -1., -2., -2., -3., -3., -4., -5., -5.],\n       [ 5., -1., -2., -3., -4., -5., -5.,  3.,  2.,  2.,  1.,  0.,  0.,\n        -1., -1., -2., -2., -3., -4., -4., -5.],\n       [ 5., -1., -2., -3., -4., -5., -5.,  3.,  3.,  2.,  1.,  1.,  0.,\n         0., -1., -1., -2., -3., -3., -4., -5.],\n       [ 5., -1., -2., -3., -4., -5., -5.,  3.,  3.,  2.,  2.,  1.,  1.,\n         0.,  0., -1., -2., -2., -3., -4., -5.],\n       [ 5., -1., -2., -3., -4., -5.,  4.,  4.,  3.,  3.,  2.,  2.,  1.,\n         1.,  0., -1., -1., -2., -3., -4., -5.],\n       [ 5., -1., -2., -3., -4., -5.,  5.,  4.,  4.,  3.,  3.,  2.,  2.,\n         1.,  0.,  0., -1., -2., -3., -4., -5.],\n       [ 5., -1., -2., -3., -4., -5.,  5.,  5.,  4.,  4.,  3.,  3.,  2.,\n         1.,  1.,  0., -1., -2., -3., -4., -5.],\n       [ 5., -1., -2., -3., -4., -5.,  5.,  5.,  5.,  4.,  4.,  3.,  2.,\n         2.,  1.,  0., -1., -2., -3., -4., -5.],\n       [ 5., -1., -2., -3., -4., -5.,  5.,  5.,  5.,  5.,  4.,  3.,  3.,\n         2.,  1.,  0., -1., -2., -3., -4., -5.],\n       [ 5., -1., -2., -3., -4., -5.,  5.,  5.,  5.,  5.,  4.,  4.,  3.,\n         2.,  1.,  0., -1., -2., -3., -4., -5.],\n       [ 5., -1., -2., -3., -4., -5.,  5.,  5.,  5.,  5.,  5.,  4.,  3.,\n         2.,  1.,  0., -1., -2., -3., -4., -5.],\n       [ 5., -1., -2., -3., -4., -5.,  5.,  5.,  5.,  5.,  5.,  4.,  3.,\n         2.,  1.,  0., -1., -2., -3., -4., -5.]])"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3116a00f86ca05b0a080e249215f011f7dfbe215",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# returns += prob * (rewards + DISCOUNT_RATE * stateValue[carsLoc1_prime, carsLoc2_prime]) \n# rewards are adjusted in order to accomodate enviornment limitations eg state is 19 so prob is\n#generated for all possible returns (0,1,2,3...10) but reward is calculated on for return =  0,1 and is 0 otherwise. Hence final probability for return = 2,3 ... is zero \n# as final prob = initial_prob(returns) * reward\n\n# first calculate value function using arbitary policy. (from each state for each possible combination of requests and returns, \n                        #calculate the total returns for a given action by calculating the possible next states and considering the values of the next states)\n# then apply a grredy search method for all actions possibe from a given state. The action resukting is a state with max value is chosen for the new policy\n#continue the above process for all the states and the respective possible actions \n# get new policy from above step \n# calculate the new values for each state and continue as above ",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}